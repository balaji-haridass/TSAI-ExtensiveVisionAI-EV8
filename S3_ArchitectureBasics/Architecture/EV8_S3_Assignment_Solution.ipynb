{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "# import all the required torch functions\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is out base model for MNIST classification\n",
        "class Net(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Each Convolution extracts feature, what feature depends on the Kernel values. Kernel is generally Odd Size.\n",
        "        # prototype => torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, etc\n",
        "\n",
        "        # Max pooling is a pooling operation that selects the maximum element from the region of the feature map covered by the filter. \n",
        "        # Thus, the output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map.\n",
        "        # The movement we do max pooling the RF(receptive field) doubles.\n",
        "\n",
        "        # Input_Size: 28x28(as per the caller in this example)\n",
        "        \n",
        "        # input: 28x28x1 => output: 22x22x32\n",
        "        self.conv1 = nn.Sequential(\n",
        "            \n",
        "            # the digits are focused around the center, so no need to maintain pixel boundary with paadding = 1\n",
        "            # Input_Size: 28x28(depends on image input) => Input_Channel: 1, Kernel: 3x3x1, Padding: 0, Output_Channels/No_Of_Kernels: 16 (3x3x1(16))=> Output: 26x26x16\n",
        "            nn.Conv2d(1, 16, 3, bias=False), # 28x28x1 -> 26x26x16 ? RF: 3x3\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            nn.Conv2d(16, 16, 3, bias=False), # 26x26x16 -> 24x24x16 ? RF: 5x5\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            nn.Conv2d(16, 32, 3, bias=False), # 24x24x16 -> 22x22x32 ? RF: 7x7\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "        # translation layer: merge features using 1x1 convolution\n",
        "        # input: 22x22x32 => output: 11x11x16\n",
        "        self.trans1 = nn.Sequential(\n",
        "        \n",
        "            nn.Conv2d(32, 16, 1, bias=False), # 22x22x32 -> 22x22x16 ? RF: 7x7 (remains same with 1x1 conv2d)\n",
        "            nn.ReLU(),\n",
        "            #nn.BatchNorm2d(16),\n",
        "\n",
        "            nn.MaxPool2d(2, 2), # 22x22x16 -> 11x11x16 ? RF: 14x14 (doubled)\n",
        "        )\n",
        "        \n",
        "        # input: 11x11x16 => output: 7x7x16\n",
        "        self.conv2 = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(16, 16, 3, bias=False), # 11x11x16 -> 9x9x16 ? RF: 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            nn.Conv2d(16, 16, 3, bias=False), # 9x9x16 -> 7x7x16 ? RF: 18x18\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "       # input: 7x7x16 => output: 5x5x16\n",
        "        self.conv3 = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(16, 16, 3, padding=1, bias=False), # 7x7x16 -> 7x7x16 ? RF: 20x20\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            nn.Conv2d(16, 16, 3, bias=False), # 7x7x16 -> 5x5x16 ? RF: 22x22 \n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.1),\n",
        "        )\n",
        "\n",
        "        # GAP Layer\n",
        "        # input: 5x5x16 => output: 1x1x10\n",
        "        self.gap_layer = nn.Sequential(\n",
        "            # 1x1 convoluation to mix 16 channels\n",
        "            nn.Conv2d(16, 10, 1, bias=False), # 5x5x16 -> 5x5x10 ? RF: 22x22\n",
        "            nn.AvgPool2d(5) # 5x5x10 -> 1x1x10 \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.gap_layer(x)\n",
        "\n",
        "        x = x.view(-1, 10) # Currently the output is 1x1x10. Flatten it to 1x10 before feeding it to Log_softmax (-1 identified the other dimension, knowing one dimension Ex.10)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "8n8bhJY7V7wf"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb113f1-4bde-473b-95e4-ad0cadcd3086"
      },
      "source": [
        "# Torch-summary provides information complementary to what is provided by print(your_model) in PyTorch, \n",
        "# similar to Tensorflow's model. summary() API to view the visualization of the model, which is helpful while debugging your network.\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "# Once we have GPU ready to access, the next step is getting PyTorch to use for storing data (tensors) and computing on data (performing operations on tensors).\n",
        "# test if PyTorch has access to a GPU\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# if GPU is availble use it, if not use CPU. allocate device and assign n/w to it\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# configure the model to be loaded on the device decided above.\n",
        "model = Net().to(device)\n",
        "\n",
        "# input image size: 28x28, Input_channels: 1\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             144\n",
            "              ReLU-2           [-1, 16, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 16, 26, 26]              32\n",
            "         Dropout2d-4           [-1, 16, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 24, 24]           2,304\n",
            "              ReLU-6           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
            "         Dropout2d-8           [-1, 16, 24, 24]               0\n",
            "            Conv2d-9           [-1, 32, 22, 22]           4,608\n",
            "             ReLU-10           [-1, 32, 22, 22]               0\n",
            "      BatchNorm2d-11           [-1, 32, 22, 22]              64\n",
            "        Dropout2d-12           [-1, 32, 22, 22]               0\n",
            "           Conv2d-13           [-1, 16, 22, 22]             512\n",
            "             ReLU-14           [-1, 16, 22, 22]               0\n",
            "        MaxPool2d-15           [-1, 16, 11, 11]               0\n",
            "           Conv2d-16             [-1, 16, 9, 9]           2,304\n",
            "             ReLU-17             [-1, 16, 9, 9]               0\n",
            "      BatchNorm2d-18             [-1, 16, 9, 9]              32\n",
            "        Dropout2d-19             [-1, 16, 9, 9]               0\n",
            "           Conv2d-20             [-1, 16, 7, 7]           2,304\n",
            "             ReLU-21             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-22             [-1, 16, 7, 7]              32\n",
            "        Dropout2d-23             [-1, 16, 7, 7]               0\n",
            "           Conv2d-24             [-1, 16, 7, 7]           2,304\n",
            "             ReLU-25             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-26             [-1, 16, 7, 7]              32\n",
            "        Dropout2d-27             [-1, 16, 7, 7]               0\n",
            "           Conv2d-28             [-1, 16, 5, 5]           2,304\n",
            "             ReLU-29             [-1, 16, 5, 5]               0\n",
            "      BatchNorm2d-30             [-1, 16, 5, 5]              32\n",
            "        Dropout2d-31             [-1, 16, 5, 5]               0\n",
            "           Conv2d-32             [-1, 10, 5, 5]             160\n",
            "        AvgPool2d-33             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 17,200\n",
            "Trainable params: 17,200\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.32\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.39\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-216-30a5a40f7c06>:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "\n",
        "#  When requested for random number in pytorch (with torch.rand() etc) these random numbers are generated from a specific algorithm. \n",
        "# A nice property of this algorithm is that we can fix it's starting point and it will always generate the same random numbers afterwards\n",
        "# seed is used to fix the starting point. This helps to start always at the same point every time we restart. Helps a lot to evaluate results taken at different times.\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# This is the batch size required for training/evaluation at a time.\n",
        "batch_size = 128\n",
        "\n",
        "# to configure the workers required to collect the data\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# Load required datasets: Kwargs help to accelerate the collection.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "# Tqdm is a library in Python which is used for creating Progress Meters or Progress Bars.\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    \n",
        "    model.train() # set the model with training mode (this is the default mode)\n",
        "\n",
        "    # load the training data and show the progress bar. Useful when data is big.\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)   # connect data and target(labels) to same device\n",
        "        optimizer.zero_grad()                     # reset all optimizer's gradients before new pass. To be recalculated for the specific training step\n",
        "        output = model(data)                      # The model goes through all of the training data in this batch once, performing its forward() function calculations. \n",
        "        loss = F.nll_loss(output, target)         # Calculate the loss, output w.r.t target(ground truth), nll - Negative Log-Likelihood Loss function - applied only on models with the softmax function as an output activation layer.\n",
        "       \n",
        "        loss.backward()                           # Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with requires_grad=True). This is known as backpropagation, hence \"backwards\".\n",
        "        optimizer.step()                          # Update the parameters with requires_grad=True with respect to the loss gradients in order to improve them\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')  # update current status \n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()    # set the model with eval mode \n",
        "    test_loss = 0\n",
        "    correct = 0 \n",
        "    with torch.no_grad():    # anything done within this context does not update gradients, making the computation efficient     \n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)     # connect data and target to same device\n",
        "            output = model(data)                                  # The model goes through all of the test data in this batch once, performing its forward() function calculations.\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # cacluate the loss w.r.t target, add up the  batch-loss as part of the total loss (lower the better)\n",
        "            pred = output.argmax(dim=1, keepdim=True)             # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # check if the predicted and target is same within the batch, if so how many (sum())\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)   # Average test loss\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee23f6e2-7612-4532-b67d-93fc10329e9c"
      },
      "source": [
        "\n",
        "# move the model to the required device, GPU if available, if not on CPU.\n",
        "model = Net().to(device)\n",
        "\n",
        "# create an optimizer (SGD) with required parameters, \n",
        "# model.parameters() - these are the model parameters to optimize, \n",
        "# learning-rate lr=0.01 and momentum-0.9\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Configure epochs to train for\n",
        "EPOCH = 20\n",
        "\n",
        "# run single epoch as of now.\n",
        "for epoch in range(1, EPOCH+1):\n",
        "    print(f'Epoch: {epoch}')\n",
        "\n",
        "    # train the model loaded on the device with the required optimizer\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # we need to evaluate on test data\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-216-30a5a40f7c06>:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.27887117862701416 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 24.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0716, Accuracy: 9788/10000 (97.88%)\n",
            "\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0994519516825676 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0420, Accuracy: 9881/10000 (98.81%)\n",
            "\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.08174566179513931 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0728725716471672 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0290, Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.04725802317261696 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 27.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0279, Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.032591983675956726 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0275, Accuracy: 9912/10000 (99.12%)\n",
            "\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.011408747173845768 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0255, Accuracy: 9917/10000 (99.17%)\n",
            "\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.04676000401377678 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0240, Accuracy: 9926/10000 (99.26%)\n",
            "\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.05633997917175293 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0212, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.06940512359142303 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0235, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.14992350339889526 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0204, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.09311527013778687 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0211, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.07263132184743881 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0227, Accuracy: 9921/10000 (99.21%)\n",
            "\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.02707207016646862 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0193, Accuracy: 9938/10000 (99.38%)\n",
            "\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.02794703282415867 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 9942/10000 (99.42%)\n",
            "\n",
            "Epoch: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.028749413788318634 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 9945/10000 (99.45%)\n",
            "\n",
            "Epoch: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.030655235052108765 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "Epoch: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0977574959397316 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0189, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "Epoch: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.05251206085085869 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "Epoch: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.03706631436944008 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 9947/10000 (99.47%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Model\n",
        "Model Results\n",
        "- Total params: 6,379,786\n",
        "- Trainable params: 6,379,786\n",
        "- Non-trainable params: 0\n",
        "- Forward/backward pass size (MB): 1.51\n",
        "- Params size (MB): 24.34\n",
        "- Estimated Total Size (MB): 25.85\n",
        "- Accuracy: 28%\n",
        "\n",
        "---\n",
        "\n",
        "# Overall Improvements Done\n",
        "\n",
        "## Improvements Done:\n",
        "- In Class Net Model\n",
        "  - Started with lower channel size, removed padding to reduce image \n",
        "  - same time consistently increased Receptive field (RF), not more than image size\n",
        "  - Removed Relu just before the end layer so that we consider both +ve & -ve values\n",
        "  - Included Batch Normalization\n",
        "  - Included Dropout of 10%\n",
        "  - Include Gap layer at the end instead of FC\n",
        "  - Used Maxpool far away from the last layer\n",
        "  - Utilized 1x1 convulations to merge results of conv2d to extract better features\n",
        "\n",
        "- In Training\n",
        "  - Increased Number of Epocs\n",
        "\n",
        "## Model Results\n",
        "- Total params: 17,200\n",
        "- Trainable params: 17,200\n",
        "- Non-trainable params: 0\n",
        "- Forward/backward pass size (MB): 1.32\n",
        "- Params size (MB): 0.07\n",
        "- Estimated Total Size (MB): 1.39\n",
        "- Accuracy: 99.47%"
      ],
      "metadata": {
        "id": "12cGla3idOXs"
      }
    }
  ]
}